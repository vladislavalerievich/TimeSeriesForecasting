data_dir: ./data
model_path: ./models
model_name: Univariate
continue_training: false
seed: 2025
debugging: false
wandb: true # whether to log to wandb


lr_scheduler: cosine
initial_lr: 0.0001 # 1e-4
learning_rate: 0.0000001 # 1e-7
scaler: custom_robust
adaptive_loss_normalization: false 
gradient_clip_val: 10000

gradient_accumulation_enabled: false
accumulation_steps: 4  # Number of batches to accumulate before updating (effective batch size = batch_size * accumulation_steps)

# total_length: 2048 # history length + future length

evaluate_on_gift_eval: true
gift_eval_max_context_length: 2048  

num_epochs: 60
num_training_iterations_per_epoch: 128
log_interval: 1

data_augmentation:
  nan_augmentation:
    enabled: true
    # Path to the root of the raw GIFT-eval datasets (e.g., 'data/gift_eval')
    # This is required to analyze the NaN distributions.
    raw_data_path: "data/gift_eval"

# TimeSeriesModel Configuration
TimeSeriesModel:
  # Core architecture
  embed_size: 128
  token_embed_dim: 256
  num_encoder_layers: 5
  
  # Scaling and preprocessing
  scaler: custom_robust
  epsilon: 0.001
  scaler_clamp_value: null
  handle_constants: false
  
  # Time features
  K_max: 6
  time_feature_config:
    use_enhanced_features: true
    use_holiday_features: false
    use_index_features: true
    include_seasonality_info: true

  # Positional encoding
  sin_pos_enc: false
  sin_pos_const: 100
  
  drop_enc_allow: false
  encoding_dropout: 0.0
  
  # Model architecture
  use_dilated_conv: false
  dilated_conv_kernel_size: 5
  dilated_conv_max_dilation: 3
  
  # Encoder configuration
  encoder_config:
    attn_mode: chunk
    num_heads: 4
    expand_v: 1.0
    use_gate: false
    use_short_conv: true
    conv_size: 4
    allow_neg_eigval: true
    use_forget_gate: true
    num_householder: 2

  loss_type: 'quantile'
  quantiles: [ 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 ]